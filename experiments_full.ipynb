{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# All-in-One Experiment Runner\n",
        "This notebook contains all necessary functions and classes to run the LoRA experiments.\n",
        "The code below is automatically loaded from the project source files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import yaml\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm.notebook import tqdm\n",
        "import json\n",
        "import os\n",
        "from datetime import datetime\n",
        "from transformers import AutoTokenizer, BertForSequenceClassification, RobertaForSequenceClassification, DistilBertForSequenceClassification\n",
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "config = {\n",
        "    'seed': 42,\n",
        "    'batch_size': 32,\n",
        "    'max_epochs': 5,\n",
        "    'learning_rate': 2e-4,\n",
        "    'lambda_reg': 0.01,\n",
        "    'scale_factor': 1.0,\n",
        "    'lora_rank': 4,\n",
        "    'lora_alpha': 1.0,\n",
        "    'dropout': 0.1,\n",
        "    'warmup_steps': 100,\n",
        "    'unfreeze_layers_after': 2\n",
        "}\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "os.makedirs(\"results\", exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Source: lora_utils/modeling.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# -------------------------------\n",
        "# 1\ufe0f\u20e3 LoRA Weighted Function\n",
        "# -------------------------------\n",
        "class LoRAWeightedFunction(torch.autograd.Function):\n",
        "    \"\"\"\n",
        "    Custom forward/backward function for LoRA layer.\n",
        "    \n",
        "    This function implements the core logic of label-wise regularization.\n",
        "    In the backward pass, it scales the gradients based on the inverse of the \n",
        "    output norm. This means samples with lower confidence (smaller output norm)\n",
        "    will have their gradients scaled up, while high-confidence samples will have\n",
        "    smaller gradients.\n",
        "    \"\"\"\n",
        "    @staticmethod\n",
        "    def forward(ctx, x, A, B, scale_factor=1.0):\n",
        "        \"\"\"\n",
        "        Forward pass: computes x @ A @ B\n",
        "        \"\"\"\n",
        "        ctx.save_for_backward(x, A, B)\n",
        "        ctx.scale_factor = scale_factor\n",
        "        out = x @ A @ B\n",
        "        ctx.out_forward = out.detach()\n",
        "        return out\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        \"\"\"\n",
        "        Backward pass: computes gradients with sample-wise scaling.\n",
        "        \"\"\"\n",
        "        x, A, B = ctx.saved_tensors\n",
        "        out = ctx.out_forward\n",
        "\n",
        "        # Compute per-sample output norm\n",
        "        out_norm = torch.norm(out, dim=-1, keepdim=True) + 1e-6\n",
        "        weight = ctx.scale_factor / out_norm\n",
        "        grad = grad_output * weight\n",
        "\n",
        "        # Sample-level gradients\n",
        "        grad_A_sample = x.unsqueeze(2) @ (grad @ B.T).unsqueeze(1)  # [B, D, r]\n",
        "        grad_B_sample = (x @ A).unsqueeze(2) * grad.unsqueeze(1)    # [B, r, D]\n",
        "\n",
        "        grad_A = grad_A_sample.sum(dim=0)\n",
        "        grad_B = grad_B_sample.sum(dim=0)\n",
        "        grad_x = grad @ B @ A.T\n",
        "\n",
        "        # Save sample-level gradients for regularization\n",
        "        # Note: This static storage is not thread-safe or multi-model safe. \n",
        "        # For production, consider attaching to the module instance or context.\n",
        "        LoRAWeightedFunction.grad_A_sample = grad_A_sample\n",
        "        LoRAWeightedFunction.grad_B_sample = grad_B_sample\n",
        "\n",
        "        return grad_x, grad_A, grad_B, None\n",
        "\n",
        "# -------------------------------\n",
        "# 2\ufe0f\u20e3 LoRA Linear Layer\n",
        "# -------------------------------\n",
        "class LoRABertLinear(nn.Module):\n",
        "    \"\"\"\n",
        "    LoRA Linear Layer that replaces a standard nn.Linear layer.\n",
        "    \n",
        "    It freezes the original weights and adds trainable LoRA matrices A and B.\n",
        "    It uses LoRAWeightedFunction for the forward pass of the LoRA path to \n",
        "    enable the gradient scaling logic.\n",
        "    \"\"\"\n",
        "    def __init__(self, original_linear, r=4, alpha=1.0, scale_factor=1.0, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.in_features = original_linear.in_features\n",
        "        self.out_features = original_linear.out_features\n",
        "        self.r = r\n",
        "        self.alpha = alpha\n",
        "        self.scale_factor = scale_factor\n",
        "        self.scaling = alpha / r\n",
        "        \n",
        "        # Freeze original weights\n",
        "        self.weight = nn.Parameter(original_linear.weight.data.clone())\n",
        "        self.weight.requires_grad = False\n",
        "        \n",
        "        # LoRA parameters\n",
        "        self.lora_A = nn.Parameter(torch.randn(self.in_features, r) * 0.01)\n",
        "        self.lora_B = nn.Parameter(torch.randn(r, self.out_features) * 0.01)\n",
        "        \n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        \n",
        "        # Buffers for gradients\n",
        "        self.grad_A_sample = None\n",
        "        self.grad_B_sample = None\n",
        "        \n",
        "        self.lora_A.register_hook(self._save_grad_A)\n",
        "        self.lora_B.register_hook(self._save_grad_B)\n",
        "\n",
        "    def _save_grad_A(self, grad):\n",
        "        self.grad_A_sample = grad\n",
        "\n",
        "    def _save_grad_B(self, grad):\n",
        "        self.grad_B_sample = grad\n",
        "\n",
        "    def forward(self, x):\n",
        "        main = x @ self.weight.T\n",
        "        lora = LoRAWeightedFunction.apply(x, self.lora_A, self.lora_B, self.scale_factor)\n",
        "        return main + self.scaling * self.dropout(lora)\n",
        "\n",
        "# -------------------------------\n",
        "# 3\ufe0f\u20e3 Injection Utility\n",
        "# -------------------------------\n",
        "def inject_lora_bert(model, r=4, alpha=1.0, scale_factor=1.0, dropout=0.1):\n",
        "    \"\"\"\n",
        "    Inject LoRA layers into a BERT-based model.\n",
        "    \n",
        "    It targets the query, key, and value projection layers in the self-attention mechanism.\n",
        "    \n",
        "    Args:\n",
        "        model (nn.Module): The model to modify.\n",
        "        r (int): LoRA rank.\n",
        "        alpha (float): LoRA alpha scaling.\n",
        "        scale_factor (float): Factor for the weighted gradient scaling.\n",
        "        dropout (float): Dropout probability.\n",
        "    \"\"\"\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, nn.Linear) and \\\n",
        "           ('query' in name or 'key' in name or 'value' in name or \\\n",
        "            'q_lin' in name or 'k_lin' in name or 'v_lin' in name):\n",
        "            # Handle both BERT and RoBERTa/DistilBERT naming conventions if possible\n",
        "            # But strictly speaking, we need to find the parent module.\n",
        "            # This simple string split works for standard Transformers models.\n",
        "            parent_name = name.rsplit('.', 1)[0]\n",
        "            child_name = name.rsplit('.', 1)[1]\n",
        "            \n",
        "            # Retrieve parent module\n",
        "            parent = model\n",
        "            for part in parent_name.split('.'):\n",
        "                parent = getattr(parent, part)\n",
        "            \n",
        "            # Replace\n",
        "            setattr(parent, child_name, LoRABertLinear(module, r, alpha, scale_factor, dropout))\n",
        "\n",
        "# -------------------------------\n",
        "# 4\ufe0f\u20e3 Regularization Loss\n",
        "# -------------------------------\n",
        "def grad_regularization_bert(model, logits, labels):\n",
        "    \"\"\"\n",
        "    Compute the gradient regularization loss.\n",
        "    \n",
        "    This loss penalizes the magnitude of gradients for samples that are correctly classified.\n",
        "    The idea is to stabilize the training by reducing updates from easy samples.\n",
        "    \n",
        "    Args:\n",
        "        model (nn.Module): The model.\n",
        "        logits (torch.Tensor): Output logits from the model [Batch, NumClasses].\n",
        "        labels (torch.Tensor): Ground truth labels [Batch].\n",
        "        \n",
        "    Returns:\n",
        "        torch.Tensor: The scalar regularization loss.\n",
        "    \"\"\"\n",
        "    preds = logits.argmax(dim=-1)\n",
        "    correct_mask = preds == labels\n",
        "    reg_loss = 0.0\n",
        "    count = correct_mask.sum().item()\n",
        "    if count == 0:\n",
        "        return torch.tensor(0., device=logits.device)\n",
        "        \n",
        "    for module in model.modules():\n",
        "        if isinstance(module, LoRABertLinear) and module.grad_A_sample is not None:\n",
        "            # We need to be careful about the batch dimension matching\n",
        "            # Assuming grad_A_sample is [B, D, r]\n",
        "            if module.grad_A_sample.shape[0] != correct_mask.shape[0]:\n",
        "                continue # Skip if shapes don't match (e.g. last batch)\n",
        "                \n",
        "            mask = correct_mask.view(-1, 1, 1).expand_as(module.grad_A_sample)\n",
        "            grad_A_correct = module.grad_A_sample[mask].view(-1, module.r)\n",
        "            \n",
        "            mask_B = correct_mask.view(-1, 1, 1).expand_as(module.grad_B_sample)\n",
        "            grad_B_correct = module.grad_B_sample[mask_B].view(-1, module.lora_B.size(1))\n",
        "            \n",
        "            reg_loss += (grad_A_correct**2).sum() + (grad_B_correct**2).sum()\n",
        "            \n",
        "    return reg_loss / count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Source: _datasets/mrpc.py\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "\n",
        "def get_dataset_mrpc(split, tokenizer, max_length=128):\n",
        "    \"\"\"\n",
        "    Load and preprocess the MRPC dataset.\n",
        "    \n",
        "    Args:\n",
        "        split (str): One of 'train', 'validation', 'test'.\n",
        "        tokenizer (PreTrainedTokenizer): Tokenizer to process the text.\n",
        "        max_length (int): Maximum sequence length.\n",
        "        \n",
        "    Returns:\n",
        "        Dataset: The tokenized dataset with 'input_ids', 'attention_mask', and 'label'.\n",
        "    \"\"\"\n",
        "    dataset = load_dataset('glue', 'mrpc', split=split)\n",
        "    \n",
        "    def tokenize_function(examples):\n",
        "        return tokenizer(examples['sentence1'], examples['sentence2'], \n",
        "                         padding='max_length', truncation=True, max_length=max_length)\n",
        "    \n",
        "    tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "    tokenized_datasets.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
        "    \n",
        "    return tokenized_datasets\n",
        "\n",
        "\n",
        "# Source: _datasets/sts_b.py\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "\n",
        "def get_dataset_stsb(split, tokenizer, max_length=128):\n",
        "    \"\"\"\n",
        "    Load and preprocess the STS-B dataset.\n",
        "    \n",
        "    Args:\n",
        "        split (str): One of 'train', 'validation', 'test'.\n",
        "        tokenizer (PreTrainedTokenizer): Tokenizer to process the text.\n",
        "        max_length (int): Maximum sequence length.\n",
        "        \n",
        "    Returns:\n",
        "        Dataset: The tokenized dataset with 'input_ids', 'attention_mask', and 'label'.\n",
        "                 Labels are converted to floats for regression.\n",
        "    \"\"\"\n",
        "    dataset = load_dataset('glue', 'stsb', split=split)\n",
        "    \n",
        "    def tokenize_function(examples):\n",
        "        return tokenizer(examples['sentence1'], examples['sentence2'], \n",
        "                         padding='max_length', truncation=True, max_length=max_length)\n",
        "    \n",
        "    tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "    \n",
        "    # STS-B is a regression task, label is float\n",
        "    tokenized_datasets = tokenized_datasets.map(lambda x: {'label': float(x['label'])})\n",
        "    tokenized_datasets.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
        "    \n",
        "    return tokenized_datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Source: models/bert_lora.py\n",
        "from transformers import BertForSequenceClassification\n",
        "from lora_utils.modeling import inject_lora_bert\n",
        "\n",
        "def build_model_bert(model_name=\"bert-base-uncased\", num_labels=1, r=4, alpha=1.0, scale_factor=1.0, dropout=0.1):\n",
        "    \"\"\"\n",
        "    Build a BERT model with LoRA layers injected.\n",
        "    \n",
        "    Args:\n",
        "        model_name (str): Name of the pre-trained BERT model.\n",
        "        num_labels (int): Number of output labels.\n",
        "        r (int): LoRA rank.\n",
        "        alpha (float): LoRA alpha.\n",
        "        scale_factor (float): Scale factor for weighted gradients.\n",
        "        dropout (float): Dropout probability.\n",
        "        \n",
        "    Returns:\n",
        "        nn.Module: The modified BERT model with LoRA layers.\n",
        "    \"\"\"\n",
        "    model = BertForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
        "    inject_lora_bert(model, r=r, alpha=alpha, scale_factor=scale_factor, dropout=dropout)\n",
        "    \n",
        "    # Freeze base model parameters\n",
        "    for param in model.bert.parameters():\n",
        "        param.requires_grad = False\n",
        "        \n",
        "    return model\n",
        "\n",
        "\n",
        "# Source: models/roberta_lora.py\n",
        "from transformers import RobertaForSequenceClassification\n",
        "from lora_utils.modeling import inject_lora_bert\n",
        "\n",
        "def build_model_roberta(model_name=\"roberta-base\", num_labels=1, r=4, alpha=1.0, scale_factor=1.0, dropout=0.1):\n",
        "    \"\"\"\n",
        "    Build a RoBERTa model with LoRA layers injected.\n",
        "    \n",
        "    Args:\n",
        "        model_name (str): Name of the pre-trained RoBERTa model.\n",
        "        num_labels (int): Number of output labels.\n",
        "        r (int): LoRA rank.\n",
        "        alpha (float): LoRA alpha.\n",
        "        scale_factor (float): Scale factor for weighted gradients..\n",
        "        dropout (float): Dropout probability.\n",
        "        \n",
        "    Returns:\n",
        "        nn.Module: The modified RoBERTa model with LoRA layers.\n",
        "    \"\"\"\n",
        "    model = RobertaForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
        "    inject_lora_bert(model, r=r, alpha=alpha, scale_factor=scale_factor, dropout=dropout)\n",
        "    \n",
        "    # Freeze base model parameters\n",
        "    for param in model.roberta.parameters():\n",
        "        param.requires_grad = False\n",
        "        \n",
        "    return model\n",
        "\n",
        "\n",
        "# Source: models/distilbert_lora.py\n",
        "from transformers import DistilBertForSequenceClassification\n",
        "from lora_utils.modeling import inject_lora_bert\n",
        "\n",
        "def build_model_distilbert(model_name=\"distilbert-base-uncased\", num_labels=1, r=4, alpha=1.0, scale_factor=1.0, dropout=0.1):\n",
        "    \"\"\"\n",
        "    Build a DistilBERT model with LoRA layers injected.\n",
        "    \n",
        "    Args:\n",
        "        model_name (str): Name of the pre-trained DistilBERT model.\n",
        "        num_labels (int): Number of output labels.\n",
        "        r (int): LoRA rank.\n",
        "        alpha (float): LoRA alpha.\n",
        "        scale_factor (float): Scale factor for weighted gradients.\n",
        "        dropout (float): Dropout probability.\n",
        "        \n",
        "    Returns:\n",
        "        nn.Module: The modified DistilBERT model with LoRA layers.\n",
        "    \"\"\"\n",
        "    model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
        "    inject_lora_bert(model, r=r, alpha=alpha, scale_factor=scale_factor, dropout=dropout)\n",
        "    \n",
        "    # Freeze base model parameters\n",
        "    for param in model.distilbert.parameters():\n",
        "        param.requires_grad = False\n",
        "        \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate(model, dataloader, device, is_regression=False):\n",
        "    \"\"\"\n",
        "    Evaluate the model on a given dataset.\n",
        "    \n",
        "    Args:\n",
        "        model (nn.Module): The model to evaluate.\n",
        "        dataloader (DataLoader): DataLoader for the evaluation dataset.\n",
        "        device (str): Device to run the evaluation on ('cuda' or 'cpu').\n",
        "        is_regression (bool): Whether the task is regression (True) or classification (False).\n",
        "        \n",
        "    Returns:\n",
        "        float: The evaluation metric (MSE for regression, Accuracy for classification).\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    criterion = nn.MSELoss() if is_regression else nn.CrossEntropyLoss()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            inputs = {k: v.to(device) for k, v in batch.items() if k != 'label'}\n",
        "            labels = batch['label'].to(device)\n",
        "            \n",
        "            outputs = model(**inputs)\n",
        "            logits = outputs.logits.squeeze() if is_regression else outputs.logits\n",
        "            \n",
        "            loss = criterion(logits, labels)\n",
        "            total_loss += loss.item() * len(labels)\n",
        "            \n",
        "            if not is_regression:\n",
        "                preds = logits.argmax(dim=-1)\n",
        "                correct += (preds == labels).sum().item()\n",
        "            \n",
        "            total += len(labels)\n",
        "            \n",
        "    avg_loss = total_loss / total\n",
        "    metric = avg_loss if is_regression else correct / total\n",
        "    return metric\n",
        "\n",
        "def train(model, train_loader, val_loader, config, device, is_regression=False):\n",
        "    \"\"\"\n",
        "    Train the model.\n",
        "    \n",
        "    Args:\n",
        "        model (nn.Module): The model to train.\n",
        "        train_loader (DataLoader): DataLoader for the training dataset.\n",
        "        val_loader (DataLoader): DataLoader for the validation dataset.\n",
        "        config (dict): Configuration dictionary.\n",
        "        device (str): Device to run the training on.\n",
        "        is_regression (bool): Whether the task is regression.\n",
        "        \n",
        "    Returns:\n",
        "        float: The best metric achieved on the validation set.\n",
        "    \"\"\"\n",
        "    model.to(device)\n",
        "    optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=float(config['learning_rate']))\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config['max_epochs'])\n",
        "    scaler = torch.cuda.amp.GradScaler()\n",
        "    criterion = nn.MSELoss() if is_regression else nn.CrossEntropyLoss()\n",
        "    \n",
        "    best_metric = float('inf') if is_regression else 0.0\n",
        "    \n",
        "    for epoch in range(config['max_epochs']):\n",
        "        model.train()\n",
        "        \n",
        "        # Optional: Unfreeze layers\n",
        "        if epoch == config.get('unfreeze_layers_after', 999):\n",
        "            print(f\"Unfreezing last {config.get('unfreeze_layers_count', 2)} layers...\")\n",
        "            # Logic to unfreeze would go here (simplified for now)\n",
        "            \n",
        "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{config['max_epochs']}\")\n",
        "        for batch in pbar:\n",
        "            inputs = {k: v.to(device) for k, v in batch.items() if k != 'label'}\n",
        "            labels = batch['label'].to(device)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            with torch.cuda.amp.autocast():\n",
        "                outputs = model(**inputs)\n",
        "                logits = outputs.logits.squeeze() if is_regression else outputs.logits\n",
        "                \n",
        "                loss_task = criterion(logits, labels)\n",
        "                \n",
        "                # Gradient regularization (only for classification for now)\n",
        "                loss_grad = torch.tensor(0., device=device)\n",
        "                if not is_regression:\n",
        "                    loss_grad = grad_regularization_bert(model, outputs.logits, labels)\n",
        "                \n",
        "                loss = loss_task + float(config['lambda_reg']) * loss_grad\n",
        "            \n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            \n",
        "            pbar.set_postfix({'loss': loss.item()})\n",
        "        \n",
        "        scheduler.step()\n",
        "        \n",
        "        # Validation\n",
        "        metric = evaluate(model, val_loader, device, is_regression)\n",
        "        print(f\"Validation {'MSE' if is_regression else 'Acc'}: {metric:.4f}\")\n",
        "        \n",
        "        # Save best model if metric improves\n",
        "        if is_regression:\n",
        "            if metric < best_metric:\n",
        "                best_metric = metric\n",
        "                torch.save(model.state_dict(), f\"results/best_model_{config['model_name']}_{config['dataset_name']}.pt\")\n",
        "        else:\n",
        "            if metric > best_metric:\n",
        "                best_metric = metric\n",
        "                torch.save(model.state_dict(), f\"results/best_model_{config['model_name']}_{config['dataset_name']}.pt\")\n",
        "                \n",
        "    return best_metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define experiments mapping\n",
        "experiments_map = [\n",
        "    ('bert-base-uncased', 'sts_b', build_model_bert),\n",
        "    ('bert-base-uncased', 'mrpc', build_model_bert),\n",
        "    ('roberta-base', 'sts_b', build_model_roberta),\n",
        "    ('roberta-base', 'mrpc', build_model_roberta),\n",
        "    ('distilbert-base-uncased', 'sts_b', build_model_distilbert),\n",
        "    ('distilbert-base-uncased', 'mrpc', build_model_distilbert),\n",
        "]\n",
        "\n",
        "results = []\n",
        "\n",
        "print(\"Starting Experiments...\")\n",
        "\n",
        "for model_name, dataset_name, build_fn in experiments_map:\n",
        "    print(f\"\\n\ud83d\ude80 Running {model_name} on {dataset_name}\")\n",
        "    \n",
        "    # Update config for current run (optional, logging purposes)\n",
        "    config['model_name'] = model_name\n",
        "    config['dataset_name'] = dataset_name\n",
        "    \n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    \n",
        "    # Load Data\n",
        "    if dataset_name == 'sts_b':\n",
        "        is_regression = True\n",
        "        num_labels = 1\n",
        "        train_data = get_dataset_stsb('train', tokenizer)\n",
        "        val_data = get_dataset_stsb('validation', tokenizer)\n",
        "    else:\n",
        "        is_regression = False\n",
        "        num_labels = 2\n",
        "        train_data = get_dataset_mrpc('train', tokenizer)\n",
        "        val_data = get_dataset_mrpc('validation', tokenizer)\n",
        "        \n",
        "    train_loader = DataLoader(train_data, batch_size=config['batch_size'], shuffle=True)\n",
        "    val_loader = DataLoader(val_data, batch_size=config['batch_size'])\n",
        "    \n",
        "    # Build Model\n",
        "    model = build_fn(\n",
        "        model_name=model_name,\n",
        "        num_labels=num_labels,\n",
        "        r=config['lora_rank'],\n",
        "        alpha=config['lora_alpha'],\n",
        "        scale_factor=config['scale_factor'],\n",
        "        dropout=config['dropout']\n",
        "    )\n",
        "    \n",
        "    # Train\n",
        "    metric = train(model, train_loader, val_loader, config, device, is_regression)\n",
        "    \n",
        "    results.append({\n",
        "        'model': model_name,\n",
        "        'dataset': dataset_name,\n",
        "        'metric': metric,\n",
        "        'type': 'MSE' if is_regression else 'Accuracy'\n",
        "    })\n",
        "    \n",
        "    # Clean up to save memory\n",
        "    del model\n",
        "    torch.cuda.empty_cache()\n",
        "    \n",
        "# Save results\n",
        "with open('results/experiment_results_notebook.json', 'w') as f:\n",
        "    json.dump(results, f, indent=4)\n",
        "    \n",
        "print(\"\\n\u2705 All experiments completed!\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}