{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# All-in-One Experiment Runner\n",
        "This notebook contains all necessary functions and classes to run the LoRA experiments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import yaml\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm.notebook import tqdm\n",
        "import json\n",
        "import os\n",
        "from datetime import datetime\n",
        "from transformers import AutoTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "config = {\n",
        "    'seed': 42,\n",
        "    'batch_size': 32,\n",
        "    'max_epochs': 5,\n",
        "    'learning_rate': 2e-4,\n",
        "    'lambda_reg': 0.01,\n",
        "    'scale_factor': 1.0,\n",
        "    'lora_rank': 4,\n",
        "    'lora_alpha': 1.0,\n",
        "    'dropout': 0.1,\n",
        "    'warmup_steps': 100,\n",
        "    'unfreeze_layers_after': 2\n",
        "}\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "os.makedirs(\"results\", exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# -------------------------------\n",
        "# 1\ufe0f\u20e3 LoRA Weighted Function\n",
        "# -------------------------------\n",
        "class LoRAWeightedFunction(torch.autograd.Function):\n",
        "    \"\"\"\n",
        "    Custom forward/backward function for LoRA layer.\n",
        "    Forward: computes x @ A @ B\n",
        "    Backward: scales gradients based on output norm to encourage learning on low-confidence samples.\n",
        "    \"\"\"\n",
        "    @staticmethod\n",
        "    def forward(ctx, x, A, B, scale_factor=1.0):\n",
        "        ctx.save_for_backward(x, A, B)\n",
        "        ctx.scale_factor = scale_factor\n",
        "        out = x @ A @ B\n",
        "        ctx.out_forward = out.detach()\n",
        "        return out\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        x, A, B = ctx.saved_tensors\n",
        "        out = ctx.out_forward\n",
        "\n",
        "        # Compute per-sample output norm\n",
        "        out_norm = torch.norm(out, dim=-1, keepdim=True) + 1e-6\n",
        "        weight = ctx.scale_factor / out_norm\n",
        "        grad = grad_output * weight\n",
        "\n",
        "        # Sample-level gradients\n",
        "        grad_A_sample = x.unsqueeze(2) @ (grad @ B.T).unsqueeze(1)  # [B, D, r]\n",
        "        grad_B_sample = (x @ A).unsqueeze(2) * grad.unsqueeze(1)    # [B, r, D]\n",
        "\n",
        "        grad_A = grad_A_sample.sum(dim=0)\n",
        "        grad_B = grad_B_sample.sum(dim=0)\n",
        "        grad_x = grad @ B @ A.T\n",
        "\n",
        "        # Save sample-level gradients for regularization\n",
        "        # Note: This static storage is not thread-safe or multi-model safe. \n",
        "        # For production, consider attaching to the module instance or context.\n",
        "        LoRAWeightedFunction.grad_A_sample = grad_A_sample\n",
        "        LoRAWeightedFunction.grad_B_sample = grad_B_sample\n",
        "\n",
        "        return grad_x, grad_A, grad_B, None\n",
        "\n",
        "# -------------------------------\n",
        "# 2\ufe0f\u20e3 LoRA Linear Layer\n",
        "# -------------------------------\n",
        "class LoRABertLinear(nn.Module):\n",
        "    def __init__(self, original_linear, r=4, alpha=1.0, scale_factor=1.0, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.in_features = original_linear.in_features\n",
        "        self.out_features = original_linear.out_features\n",
        "        self.r = r\n",
        "        self.alpha = alpha\n",
        "        self.scale_factor = scale_factor\n",
        "        self.scaling = alpha / r\n",
        "        \n",
        "        # Freeze original weights\n",
        "        self.weight = nn.Parameter(original_linear.weight.data.clone())\n",
        "        self.weight.requires_grad = False\n",
        "        \n",
        "        # LoRA parameters\n",
        "        self.lora_A = nn.Parameter(torch.randn(self.in_features, r) * 0.01)\n",
        "        self.lora_B = nn.Parameter(torch.randn(r, self.out_features) * 0.01)\n",
        "        \n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        \n",
        "        # Buffers for gradients\n",
        "        self.grad_A_sample = None\n",
        "        self.grad_B_sample = None\n",
        "        \n",
        "        self.lora_A.register_hook(self._save_grad_A)\n",
        "        self.lora_B.register_hook(self._save_grad_B)\n",
        "\n",
        "    def _save_grad_A(self, grad):\n",
        "        self.grad_A_sample = grad\n",
        "\n",
        "    def _save_grad_B(self, grad):\n",
        "        self.grad_B_sample = grad\n",
        "\n",
        "    def forward(self, x):\n",
        "        main = x @ self.weight.T\n",
        "        lora = LoRAWeightedFunction.apply(x, self.lora_A, self.lora_B, self.scale_factor)\n",
        "        return main + self.scaling * self.dropout(lora)\n",
        "\n",
        "# -------------------------------\n",
        "# 3\ufe0f\u20e3 Injection Utility\n",
        "# -------------------------------\n",
        "def inject_lora_bert(model, r=4, alpha=1.0, scale_factor=1.0, dropout=0.1):\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, nn.Linear) and \\\n",
        "           ('query' in name or 'key' in name or 'value' in name or \\\n",
        "            'q_lin' in name or 'k_lin' in name or 'v_lin' in name):\n",
        "            # Handle both BERT and RoBERTa/DistilBERT naming conventions if possible\n",
        "            # But strictly speaking, we need to find the parent module.\n",
        "            # This simple string split works for standard Transformers models.\n",
        "            parent_name = name.rsplit('.', 1)[0]\n",
        "            child_name = name.rsplit('.', 1)[1]\n",
        "            \n",
        "            # Retrieve parent module\n",
        "            parent = model\n",
        "            for part in parent_name.split('.'):\n",
        "                parent = getattr(parent, part)\n",
        "            \n",
        "            # Replace\n",
        "            setattr(parent, child_name, LoRABertLinear(module, r, alpha, scale_factor, dropout))\n",
        "\n",
        "# -------------------------------\n",
        "# 4\ufe0f\u20e3 Regularization Loss\n",
        "# -------------------------------\n",
        "def grad_regularization_bert(model, logits, labels):\n",
        "    preds = logits.argmax(dim=-1)\n",
        "    correct_mask = preds == labels\n",
        "    reg_loss = 0.0\n",
        "    count = correct_mask.sum().item()\n",
        "    if count == 0:\n",
        "        return torch.tensor(0., device=logits.device)\n",
        "        \n",
        "    for module in model.modules():\n",
        "        if isinstance(module, LoRABertLinear) and module.grad_A_sample is not None:\n",
        "            # We need to be careful about the batch dimension matching\n",
        "            # Assuming grad_A_sample is [B, D, r]\n",
        "            if module.grad_A_sample.shape[0] != correct_mask.shape[0]:\n",
        "                continue # Skip if shapes don't match (e.g. last batch)\n",
        "                \n",
        "            mask = correct_mask.view(-1, 1, 1).expand_as(module.grad_A_sample)\n",
        "            grad_A_correct = module.grad_A_sample[mask].view(-1, module.r)\n",
        "            \n",
        "            mask_B = correct_mask.view(-1, 1, 1).expand_as(module.grad_B_sample)\n",
        "            grad_B_correct = module.grad_B_sample[mask_B].view(-1, module.lora_B.size(1))\n",
        "            \n",
        "            reg_loss += (grad_A_correct**2).sum() + (grad_B_correct**2).sum()\n",
        "            \n",
        "    return reg_loss / count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "import torch\n",
        "\n",
        "def get_dataset_mrpc(split, tokenizer, max_length=128):\n",
        "    \"\"\"\n",
        "    Load MRPC dataset.\n",
        "    split: 'train', 'validation', 'test'\n",
        "    \"\"\"\n",
        "    dataset = load_dataset('glue', 'mrpc', split=split)\n",
        "    \n",
        "    def tokenize_function(examples):\n",
        "        return tokenizer(examples['sentence1'], examples['sentence2'], \n",
        "                         padding='max_length', truncation=True, max_length=max_length)\n",
        "    \n",
        "    tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "    tokenized_datasets.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
        "    \n",
        "    return tokenized_datasets\n",
        "\n",
        "def get_dataset_stsb(split, tokenizer, max_length=128):\n",
        "    \"\"\"\n",
        "    Load STS-B dataset.\n",
        "    split: 'train', 'validation', 'test'\n",
        "    \"\"\"\n",
        "    dataset = load_dataset('glue', 'stsb', split=split)\n",
        "    \n",
        "    def tokenize_function(examples):\n",
        "        return tokenizer(examples['sentence1'], examples['sentence2'], \n",
        "                         padding='max_length', truncation=True, max_length=max_length)\n",
        "    \n",
        "    tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "    \n",
        "    # STS-B is a regression task, label is float\n",
        "    tokenized_datasets = tokenized_datasets.map(lambda x: {'label': float(x['label'])})\n",
        "    tokenized_datasets.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
        "    \n",
        "    return tokenized_datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import BertForSequenceClassification, RobertaForSequenceClassification, DistilBertForSequenceClassification\n",
        "\n",
        "def build_model_bert(model_name=\"bert-base-uncased\", num_labels=1, r=4, alpha=1.0, scale_factor=1.0, dropout=0.1):\n",
        "    model = BertForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
        "    inject_lora_bert(model, r=r, alpha=alpha, scale_factor=scale_factor, dropout=dropout)\n",
        "    \n",
        "    # Freeze base model parameters\n",
        "    for param in model.bert.parameters():\n",
        "        param.requires_grad = False\n",
        "        \n",
        "    return model\n",
        "\n",
        "def build_model_roberta(model_name=\"roberta-base\", num_labels=1, r=4, alpha=1.0, scale_factor=1.0, dropout=0.1):\n",
        "    model = RobertaForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
        "    inject_lora_bert(model, r=r, alpha=alpha, scale_factor=scale_factor, dropout=dropout)\n",
        "    \n",
        "    # Freeze base model parameters\n",
        "    for param in model.roberta.parameters():\n",
        "        param.requires_grad = False\n",
        "        \n",
        "    return model\n",
        "\n",
        "def build_model_distilbert(model_name=\"distilbert-base-uncased\", num_labels=1, r=4, alpha=1.0, scale_factor=1.0, dropout=0.1):\n",
        "    model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
        "    inject_lora_bert(model, r=r, alpha=alpha, scale_factor=scale_factor, dropout=dropout)\n",
        "    \n",
        "    # Freeze base model parameters\n",
        "    for param in model.distilbert.parameters():\n",
        "        param.requires_grad = False\n",
        "        \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate(model, dataloader, device, is_regression=False):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    criterion = nn.MSELoss() if is_regression else nn.CrossEntropyLoss()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            inputs = {k: v.to(device) for k, v in batch.items() if k != 'label'}\n",
        "            labels = batch['label'].to(device)\n",
        "            \n",
        "            outputs = model(**inputs)\n",
        "            logits = outputs.logits.squeeze() if is_regression else outputs.logits\n",
        "            \n",
        "            loss = criterion(logits, labels)\n",
        "            total_loss += loss.item() * len(labels)\n",
        "            \n",
        "            if not is_regression:\n",
        "                preds = logits.argmax(dim=-1)\n",
        "                correct += (preds == labels).sum().item()\n",
        "            \n",
        "            total += len(labels)\n",
        "            \n",
        "    avg_loss = total_loss / total\n",
        "    metric = avg_loss if is_regression else correct / total\n",
        "    return metric\n",
        "\n",
        "def train(model, train_loader, val_loader, config, device, is_regression=False):\n",
        "    model.to(device)\n",
        "    optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=float(config['learning_rate']))\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config['max_epochs'])\n",
        "    scaler = torch.cuda.amp.GradScaler()\n",
        "    criterion = nn.MSELoss() if is_regression else nn.CrossEntropyLoss()\n",
        "    \n",
        "    best_metric = float('inf') if is_regression else 0.0\n",
        "    \n",
        "    for epoch in range(config['max_epochs']):\n",
        "        model.train()\n",
        "        \n",
        "        # Optional: Unfreeze layers\n",
        "        if epoch == config.get('unfreeze_layers_after', 999):\n",
        "            print(f\"Unfreezing last {config.get('unfreeze_layers_count', 2)} layers...\")\n",
        "            # Logic to unfreeze would go here\n",
        "            \n",
        "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{config['max_epochs']}\")\n",
        "        for batch in pbar:\n",
        "            inputs = {k: v.to(device) for k, v in batch.items() if k != 'label'}\n",
        "            labels = batch['label'].to(device)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            with torch.cuda.amp.autocast():\n",
        "                outputs = model(**inputs)\n",
        "                logits = outputs.logits.squeeze() if is_regression else outputs.logits\n",
        "                \n",
        "                loss_task = criterion(logits, labels)\n",
        "                \n",
        "                # Gradient regularization (only for classification for now)\n",
        "                loss_grad = torch.tensor(0., device=device)\n",
        "                if not is_regression and config.get('lambda_reg', 0.0) > 0.0:\n",
        "                    try:\n",
        "                        loss_grad = grad_regularization_bert(model, outputs.logits, labels)\n",
        "                    except Exception as e:\n",
        "                        # Fallback if algo is unstable\n",
        "                        pass\n",
        "                \n",
        "                loss = loss_task + float(config['lambda_reg']) * loss_grad\n",
        "            \n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            \n",
        "            pbar.set_postfix({'loss': loss.item()})\n",
        "        \n",
        "        scheduler.step()\n",
        "        \n",
        "        # Validation\n",
        "        metric = evaluate(model, val_loader, device, is_regression)\n",
        "        print(f\"Validation {'MSE' if is_regression else 'Acc'}: {metric:.4f}\")\n",
        "        \n",
        "        # Save best\n",
        "        if is_regression:\n",
        "            if metric < best_metric:\n",
        "                best_metric = metric\n",
        "                torch.save(model.state_dict(), f\"results/best_model_{config['model_name']}_{config['dataset_name']}.pt\")\n",
        "        else:\n",
        "            if metric > best_metric:\n",
        "                best_metric = metric\n",
        "                torch.save(model.state_dict(), f\"results/best_model_{config['model_name']}_{config['dataset_name']}.pt\")\n",
        "                \n",
        "    return best_metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define experiments mapping\n",
        "experiments_map = [\n",
        "    ('bert-base-uncased', 'sts_b', build_model_bert),\n",
        "    ('bert-base-uncased', 'mrpc', build_model_bert),\n",
        "    ('roberta-base', 'sts_b', build_model_roberta),\n",
        "    ('roberta-base', 'mrpc', build_model_roberta),\n",
        "    ('distilbert-base-uncased', 'sts_b', build_model_distilbert),\n",
        "    ('distilbert-base-uncased', 'mrpc', build_model_distilbert),\n",
        "]\n",
        "\n",
        "results = []\n",
        "\n",
        "print(\"Starting Experiments...\")\n",
        "\n",
        "for model_name, dataset_name, build_fn in experiments_map:\n",
        "    print(f\"\\n\ud83d\ude80 Running {model_name} on {dataset_name}\")\n",
        "    \n",
        "    # Update config for current run (optional, logging purposes)\n",
        "    config['model_name'] = model_name\n",
        "    config['dataset_name'] = dataset_name\n",
        "    \n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    \n",
        "    # Load Data\n",
        "    if dataset_name == 'sts_b':\n",
        "        is_regression = True\n",
        "        num_labels = 1\n",
        "        train_data = get_dataset_stsb('train', tokenizer)\n",
        "        val_data = get_dataset_stsb('validation', tokenizer)\n",
        "    else:\n",
        "        is_regression = False\n",
        "        num_labels = 2\n",
        "        train_data = get_dataset_mrpc('train', tokenizer)\n",
        "        val_data = get_dataset_mrpc('validation', tokenizer)\n",
        "        \n",
        "    train_loader = DataLoader(train_data, batch_size=config['batch_size'], shuffle=True)\n",
        "    val_loader = DataLoader(val_data, batch_size=config['batch_size'])\n",
        "    \n",
        "    # Build Model\n",
        "    model = build_fn(\n",
        "        model_name=model_name,\n",
        "        num_labels=num_labels,\n",
        "        r=config['lora_rank'],\n",
        "        alpha=config['lora_alpha'],\n",
        "        scale_factor=config['scale_factor'],\n",
        "        dropout=config['dropout']\n",
        "    )\n",
        "    \n",
        "    # Train\n",
        "    metric = train(model, train_loader, val_loader, config, device, is_regression)\n",
        "    \n",
        "    results.append({\n",
        "        'model': model_name,\n",
        "        'dataset': dataset_name,\n",
        "        'metric': metric,\n",
        "        'type': 'MSE' if is_regression else 'Accuracy'\n",
        "    })\n",
        "    \n",
        "    # Clean up to save memory\n",
        "    del model\n",
        "    torch.cuda.empty_cache()\n",
        "    \n",
        "# Save results\n",
        "with open('results/experiment_results_notebook.json', 'w') as f:\n",
        "    json.dump(results, f, indent=4)\n",
        "    \n",
        "print(\"\\n\u2705 All experiments completed!\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}